---
title: "Final Project Summary: A staRz is Born"
author: "Brady Nifong, Ethan Alt, Hillary Heiling, Nate Bean, Sarah Reifeis"
date: "April 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\usepackage{amsmath}

```{r, message=FALSE}
library(staRz)
library(devtools)
devtools::document("staRz")
```

## Introduction to Our Data

### Raw Data
We found the dataset that inspired our project on Kaggle, titled [Consumer Reviews of Amazon Products](https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products). This dataset contains 34,660 reviews of Amazon products such as the Kindle, Fire TV Stick, etc. across multiple different stores (e.g., Amazon, Best Buy). In particular, the variables of interest in this dataset are the number of stars the product received from each reviewer, the title of each review, and the text of each review. 

All variables that were used to build our models needed to be derived from this dataset, so we spend some time in the following sections describing the approaches we took to juicing useful information out of this raw data.

### Variable Creation

The dataset contained two text variables we were interested in - reviews.text and reviews.title. The two variables themselves contain a wealth of information but not in a useful format, so instead we had to mine them to get useful data. Below we discuss further how we decided to derive both "human-made" and "machine-made" variables from the raw text of the Amazon reviews.

#### "Human-Made" Variables

For each of the text variables, we were interested in word count, proportion of positive words and proportion of negative words. We wrote functions to create these variables, as well as the redundant variables positive word count and negative word count. 

Before we could create the variables, we had to clean reviews.text and reviews.title so that we could properly match the words in the text variables to positive and negative sentiments. To do this, we called the `cleanSentence` function, which converts the sentence to a character in case it is of variable type factor, then it makes the sentence lower case, and finally it removes all punctuation from the sentence. 

Once the sentence is clean, we use the ngram function `wordcount` to get the number of words in each sentence. Next we use our function `getSentCount` to create a vector of words in the sentence. From there we count the number of words in the sentence that are positive and the number that are negative. We also calculate the proportion of positive words and proportion of negative words.

The function `getCountData` is then used as a wrapper for all of the above functions, as well as the functions that retrieve the positive (`getPos`) and negative words (`getNeg`). 

```{r, echo = FALSE}
clean_reviews <- read.csv("/Users/nathanbean/Documents/Spring 2019/BIOS 735/Final Project/merged.csv",header=T)
```

We also dichotomized the review.rating variable, which had values NA, 1, 2, 3, 4, 5, in order to use logistic regression. Values of 1, 2, 3 were dichomotmized to 0 and values of 4, 5 were dichotomized to 1 using the function `dichotomizeStarz`. Note that `dichotomizeStarz` only takes in non-missing values, as we used the function after getting rid of all missing values.

```{r}
clean_reviews$binary_rating <- rep(NA, length(clean_reviews$reviews.rating))

missing_rate <- which(is.na(clean_reviews$reviews.rating))

clean_reviews$binary_rating[-missing_rate] <- dichotomizeStarz(clean_reviews$reviews.rating[-missing_rate])
```

#### Investigating the Human-Made Variables

We see below that a large number of observations don't have any negative words for reviews.title or reviews.text. However, this makes some logical sense because over 90% of the observations are rated 1 (i.e. 4 or 5 stars), and we would expect higher ratings to have little to no negative words. 

It may be important to note that the distribution of word count for reviews.text and reviews.title is highly skewed. However, prior to the LASSO algorithm we scaled and centered the data which helped reduce the skewness.

We chose to use positive/negative proportion rather than count because, as stated above, some reviews have more words than others, and those reviews are more likely to have more positive/negative words simply because they have more words. Using proportion allowed us to compare the negative/positive content between different reviews on the same scale.

```{r}
## Summary for the review.text variables
summary(clean_reviews$text_word_ct)
summary(clean_reviews$text_pos_prop)
sum(clean_reviews$text_pos_ct == 0 & !is.na(clean_reviews$text_pos_ct))/(sum(!is.na(clean_reviews$text_pos_ct)))
summary(clean_reviews$text_neg_prop)
sum(clean_reviews$text_neg_ct == 0 & !is.na(clean_reviews$text_neg_ct))/(sum(!is.na(clean_reviews$text_neg_ct)))

## Summary for the review.title variables
summary(clean_reviews$title_word_ct)
summary(clean_reviews$title_pos_prop)
sum(clean_reviews$title_pos_ct == 0 & !is.na(clean_reviews$title_pos_ct))/(sum(!is.na(clean_reviews$title_pos_ct)))
summary(clean_reviews$title_neg_prop)
sum(clean_reviews$title_neg_ct == 0 & !is.na(clean_reviews$title_neg_ct))/(sum(!is.na(clean_reviews$title_neg_ct)))

denom <- sum(table(clean_reviews$binary_rating))
table(clean_reviews$binary_rating)/denom
```

#### "Machine-Made" Variables

Several data-cleaning tasks were necessary before applying LDA to our chosen dataset. First, all words in the reviews were changed to lowercase. This is because some people mistakenly capitalize letters incorrectly and `R` is a case-sensitive language. Moreover, punctuation was removed from the data. For example, the word *don't* was changed to *dont*. This is because some reviewers do not add the apostrophe and because R would interpret end-of-sentence words as distinct from middle of sentence words, since end of sentence words would have a period included.

Various *stop words* were defined. These stop words are words that get removed from the data. We removed product-specific words so that sentiment could be better captured and the topics would not be too influenced by the products. We also removed some words that could be irrelevant to sentiment. 

The stop words were: "amazon", "alexa", "echo", "tablet", "kindle", "fire", "alexia", "read", "reading", "ipad", "husband", "son", "daughter", "kids", "kid".

After performing this data-cleaning we derived our "machine-made" variables using Latent Dirichlet Allocation, which will be discussed in detail in the Methods section. If you're interested in the code for pre-processing these variables and running the LDA, please see *data_cleaning_ea.R* in our [GitHub final project repository](https://github.com/sreifeis/maRkov-chain-gang). Ultimately, we used LDA to create 5 variables from the titles of the reviews and 20 variables from the text of the reviews.

### Missing Data

Once we created all necessary variables, we realized we had some missing values. Specifically, 1,261 of the observations (about 3.6%) had a missing value for number of stars or had at least one derived covariate that was missing. Since this is such a small portion of the overall dataset, we decided to delete these observations and do a complete case analysis. Once these missing observations were removed, we were left with 33,399 observations which were allocated in a 1:1 ratio to the training and test sets for use in our models. 

### Packaging

Our R package `staRz` contains the following functions and datasets:

* Functions for human-created variables
    + `cleanSentence`
    + `dichotomizeStarz`
    + `getCountData`
    + `getNeg`
    + `getPos`
    + `getSentCount`
* LASSO functions
    + `LQA.lasso`
    + `soft.thresh`
* Datasets
    + `test`
    + `train`

The `test` and `train` datasets each contain a randomly sampled 50% of the complete case data.
```{r}
dim(test)
dim(train)
```

Both datasets contain the design matrix (1 intercept column & 31 derived variables) and the outcome vector for use in the `LQA.lasso` function. The 31 derived variables in these datasets have already been centered and scaled (both with the means and standard deviations from the training set) and are ready for use in model fitting.

All functions have documentation describing their inputs and a basic description of the task they perform, as well as the output they generate; the functions each have some built-in tests as well to verify that the user-supplied inputs are in the proper form and for some functions the output is tested to confirm the returned result is as expected. The datasets are also documented in the package with descriptions of the structure and the variables they contain.  

## Methods

### Latent Dirichlet Allocation (LDA)

Latent Dirichlet Allocation (LDA) is a natural language processing (NLP) technique used to model words in documents. LDA assumes that words are generated from a mixture model of latent topics. The number of topics is assumed to be known in advance. The distribution of topics is assumed to have a sparse Dirichlet prior, which gives the intuition that documents cover only a subset of topics. In addition, it encodes the assumption that topics consist of only a few words from a vocabulary.

#### Model

LDA assumes the followikng process for a corpus $D$ consisting of $M$ documents with $K$ topics and $N_i$ words for each document:

1. Choose $\theta_i \sim \text{Dir} (\alpha)$ where $i \in \{1, 2, \ldots, M\}$ and $\alpha$ is typically sparse.

2. Choose $\phi_k \sim \text{Dir}(\beta)$, where $k \in \{1, 2, \ldots, K \}$ and $\beta$ is typically sparse.

3. For each word position $i \in \{1, 2, \ldots, M \}$, $j \in \{1, 2, \ldots, N_i \}$

  + Choose a topic $z_{ij} \sim \text{Multinomial}(\theta_i)$
    
  + Choose a word $w_{ij} \sim \text{Multinomial}(\phi_{z_{ij}})$

Since the topics are latent, the model is solved by variational Bayes. Other solutions exist, including Gibbs Sampling and Expectation Propagation, but we used variational Bayes in order to speed up the process.

#### Specifics of Our LDA

We assumed that we had 20 topics for the review text and 5 topics for the title text. The `R` package topicmodels contains a function `LDA` that runs the model. We left all options as defaults, which corresponds to estimating the $\alpha$ and $\beta$ parameters. The posterior means of the topics across the documents were utilized in order to reduce the dimensionality of the data, and were utilized as predictors in the logistic regression models. 

Note that the probabilities of the 20 review text topics all sum to 1, and the same is true for the 5 title text topics. Including all of these as candidate variables in the models would cause a linear dependence issue. Our solution was to logit-transform these probabilities, allowing us to keep them all as candidate variables for the models. 


### LQA LASSO Algorithm

In this project, we used penalized logistic regression to chose a prediction model. In this penalized logistic regression, the estimates $\hat\beta$ were found by minimizing the following:

$$ \hat \beta = argmin_\beta \space \left (-\frac{1}{n}\ell(\beta) + \lambda \sum_{j=1}^p |\beta_j| \right) $$

where $\ell(\beta) = \sum_{i=1}^n \ell_i(\beta)$ is the log likelihood for the model and $\beta_j$ is the coefficient parameter associated with the jth covariate. 

Note: This minimization was achieved through coordinate descent. In coordinate descent, we solve for the new estimate of $\beta_j$ by assuming that all other $\beta_k$ for $k \ne j$ are fixed at their last updated values; i.e., the "current" values of $\beta_k$. (This algorithm will be discussed in more detail later).

In order to simplify this minimization problem, we first approximated $\ell(\beta)$ using a second order Taylor series expansion---the local quadratic approximation (LQA)---about $x_i \tilde \beta$ where $x_i = [x_{i1},...,x_{ip}]$ is the $1 \times p$ row vector of covariate values for individual $i$ and $\tilde \beta$ is the current value (i.e., last updated value) of the $\beta = [\beta_1, ..., \beta_p]^T$ $p \times 1$ vector of coefficients associated with each of the $p$ covariates. The covariates $x_{ij}$ are scaled so that $\sum_{i=1}^n x_{ij} = 0$ and $\sum_{i=1}^n x_{ij}^2 = 1$ (mean = 0 and variance = 1 for each covariate). The local quadratic approximation is detailed below:

$$ \ell_i(\beta) \approx \ell_i(\tilde \beta) + \ell_i'(\tilde \beta)(x_i\beta - x_i\tilde\beta) +
\ell_i''(\tilde\beta) (x_i\beta - x_i\tilde\beta)^2 $$

where

$$ \ell_i(\beta) = y_i(x_i\beta) - log(1 + exp(x_i\beta)), $$
$$ \ell_i'(\beta) = \frac{\partial}{\partial(x_i\beta)} \ell_i(\beta) = y_i - \frac{exp(x_i\beta)}{1 + exp(x_i\beta)}  $$
and

$$ \ell_i''(\beta) = \frac{\partial}{\partial(x_i\beta)} \ell_i'(\beta) = \left(\frac{exp(x_i\beta)}{1 + exp(x_i\beta)} \right)^2 - \frac{exp(x_i\beta)}{1 + exp(x_i\beta)} = -p(x_i)(1 - p(x_i)) $$

where 

$$p(x_i) = \frac{exp(x_i\beta)}{1 + exp(x_i\beta)}.$$

In later calculations that include $\tilde\beta$, we will denote

$$ \tilde p(x_i) = \frac{exp(x_i\tilde\beta)}{1 + exp(x_i\tilde\beta)}. $$

Using a complete-the-square approach, we can approximate $\ell_i(\beta)$ as follows:

$$ \ell_i(\beta) \approx -\frac{1}{2} \tilde p(x_i)(1-\tilde p(x_i))\left(\frac{y_i - \tilde p(x_i)}{\tilde p(x_i)(1-\tilde p(x_i))} + x_i\tilde\beta - x_i\beta \right)^2 + c(\tilde\beta) $$

where $c(\tilde\beta)$ is some constant that may depend on the data and $\tilde\beta$, but does not depend on $\beta$. Since the partial derivatives (detailed later) taken with respect to $\beta_j$ of this constant term will be zero, this constant term can be ignored.

Consequently, minimizing the negative log likelihood is equivalent to minimizing the following:

$$ -\ell(\beta) \approx \frac{1}{2} \sum_{i=1}^n \tilde p(x_i)(1-\tilde p(x_i))\left(\frac{y_i - \tilde p(x_i)}{\tilde p(x_i)(1-\tilde p(x_i))} + x_i\tilde\beta - x_i\beta \right)^2 $$

For notational convenience we can re-express some of the above terms as follows:

The weight $w_i$:

$$ w_i = \tilde p(x_i)(1-\tilde p(x_i)). $$
The working response $\tilde y_i$:

$$ \tilde y_i = \frac{y_i - \tilde p(x_i)}{\tilde p(x_i)(1-\tilde p(x_i))} + x_i\tilde\beta. $$

Consequently, the $\hat \beta$ solution of interest minimizes the following:

$$ \hat \beta = argmin_\beta \space \frac{1}{2n} \sum_{i=1}^n w_i (\tilde y_i - x_i\beta)^2 + \lambda \sum_{j=1}^p |\beta_j|. $$

After going through all of the above steps, we have now redefined the original minimization problem as the minimization of a penalized weighted least squares problem.

#### Coordinate Descent Algorithm (for LASSO)

1. **Outer Loop**: Loop over the range of potential $\lambda$ from $\lambda_{max}$ to $\lambda_{min}$ (where $\lambda$ values are equally spaced on the log-scale). For notational purposes, let $\lambda^s$ equal the $s^{th}$ $\lambda$ in the sequence. 

  + If $\lambda = \lambda_{max} = \lambda^0$, set the initial values of $\beta^{(0)}$ to be 0 for all covariates.
  
  + For each following $\lambda^s$ value ($s > 0$) in the sequence, use the previous $\beta$ solution from $\lambda^{s-1}$ as the initial values of $\beta$.
    
    2. **Middle Loop**: For each iteration $t$, Set the "current" values of $\tilde\beta$ to $\beta^{(t-1)}$, the latest updated $\beta$ values found in the previous iteration.
    
        **Inner Loop**: For $j = 1,...,p$
        
        A. Update $\beta_j$ by (a) fixing all other $\beta_k$ for $k \ne j$ to be equal to $\tilde\beta_k$, the $\beta_k$ value from the "current" values of $\tilde\beta$ and then (b) solving the minimization problem for $\beta_j$ (described later).
        
        B. Update $\tilde\beta_j$ to equal the newly found $\hat \beta_j$
    
    3. Once each $\beta_j$ is updated for j=1,...,p, let the new updated version of $\beta$ equal $\beta^{(t)}$. 
    
    4. Check if $||\beta^{(t)} - \beta^{(t-1)}|| < \epsilon$ for some small tolerance level $\epsilon$. If true, then stop. If false, then go back to "Middle Loop" and repeat steps 2 - 4 until convergence.
  
To update each $\hat \beta_j$, we note the following:

$$ \frac{\partial}{\partial\beta_j} \left ( \frac{1}{2n} \sum_{i=1}^n w_i (\tilde y_i - x_i\beta )^2 + \lambda \sum_{j=1}^p |\beta_j| \right ) $$ 

$$ = \frac{\partial}{\partial \beta_j} \left ( \frac{1}{2n} \sum_{i=1}^n w_i (\tilde y_i - x_{i-j}\tilde\beta_{-j} - x_{ij}\beta_j )^2 \right ) + \lambda \partial(|\beta_j|) $$
(where $\partial(|\beta_j|)$ is the subdifferential of $|\beta_j|$, $x_{i-j}$ is the covariate vector $x_i$ without the $j^{th}$ covariate, and $\beta_{-j}$ is the $\beta$ vector without the $j^{th}$ coefficient parameter)

$$ = -\frac{1}{n} \sum_{i=1}^n w_i x_{ij} (\tilde y_i - x_{i-j}\tilde\beta_{-j}) + \frac{1}{n} \sum_{i=1}^n w_i x_{ij}^2 \beta_j + \lambda \partial(|\beta_j|) $$

$$ = -z_j + v_j \beta_j + \lambda \partial(|\beta_j|) $$

where

$$ z_j = \frac{1}{n} \sum_{i=1}^n w_i x_{ij} (\tilde y_i - x_{i-j}\tilde\beta_{-j}), $$

$$ v_j = \frac{1}{n} \sum_{i=1}^n w_i x_{ij}^2. $$

Solving for $\beta_j$, we get the following closed-form solution:

$$ \hat \beta_j = \frac{S(z_j,\lambda)}{v_j} = \frac{sign(z_j)(|z_j| - \lambda)_+}{v_j} =  \begin{cases}
  \frac{z_j - \lambda}{v_j} & \text{if } z_j > 0 \text{ and } \lambda < |z_j| \\
  \frac{z_j + \lambda}{v_j} & \text{if } z_j < 0 \text{ and } \lambda < |z_j| \\
  0 & \text{if } \lambda \ge |z_j|
\end{cases}. $$

The paper by Breheny and Huang (2011) specifies how to find $\lambda_{max}$ when we are including an intercept in the model. First, an intercept-only model (using some standard maximization tool) is fit to the data. Then, we set $\tilde\beta = [\beta_0, 0, ..., 0]$ where $\beta_0$ is the intercept from the fit model. We use this $\tilde\beta$ to calculate $z_j$ (expression given above) for all j. Then, $\lambda_{max} = max_j|z_j|$. We then define $\lambda_{min} = \lambda_{max} \epsilon$ for some small value of $\epsilon > 0$.

The "best" $\lambda$ will be the $\lambda$ with the lowest extended BIC (Chen & Chen, 2008). The extended BIC is as follows:

$$ EBIC_\lambda = -2 \ell(\hat\beta(\lambda)) + s_\lambda log(n) + 2 s_\lambda \gamma log(P) $$

where $s_\lambda$ is the number of covariates not penalized to zero during the LASSO coordinate descent algorithm for a particular value of $\lambda$, $n$ is the number of observations, $P$ is the number of original covariates, $\gamma$ is some constant between 0 and 1 (in this case, 0.5 from recommendations from Chen & Chen), and $\hat\beta(\lambda)$ is the calculated $\beta$ values after the penalized LASSO regression procedure at the specified value of $\lambda$. Note that if we were to set $\gamma = 0$, this would reduce to the usual BIC (i.e. this extended BIC - EBIC - adds an additional penalty to the usual BIC). According to Chen & Chen (2008), this EBIC performs better in high dimensional settings (e.g., it reduceds the false discovery rate (FDR)).

Once the best $\lambda$ is found, the final $\beta$ we will use for the model will be the $\beta$ calculated during the coordinate descent algorithm at this value of $\lambda$. 


## Results

We can use the training data `train` from our `staRz` package as input to the `LQA.lasso` function.
```{r}
X.train <- train[, -33]
y.train <- train[, 33]
X.test <- test[, -33]
y.test <- test[, 33]
```

The LASSO algorithm tests 93 values of $\lambda$ which range between 0.0016 and 0.1624.

```{r}
### Determine sequence of lambda values to be tested ###

library(stats) # For 'glm'
fit = glm(y.train ~ 1, family = binomial)

# Extract intercept
int = fit$coefficients
B_test = c(int, numeric((ncol(X.train) - 1)))
prob = exp(int) / (1 + exp(int))
prob.vec <- rep(prob, nrow(train))

# Find lambda_max
W = prob*(1-prob)  # Weight when B = 0 (other than intercept)
y_wr = (y.train - prob) / (prob * (1-prob)) + X.train %*% B_test  # Working response for intercept-only results

lam_option = numeric(ncol(X.train))
for(j in 1:ncol(X.train)){
  lam_option[j] = abs( (1/nrow(X.train)) * sum( X.train[,j] * W * (y_wr - X.train[,-j] %*% B_test[-j]) ) )
}

lambda_max = max(lam_option)
epsilon = 0.01
lambda_min = lambda_max * epsilon

# Recommendation by 761 notes: perform sequence on log scale
log_lam = seq(from = log(lambda_max), to = log(lambda_min), by = -0.05)
# Re-transform back to regular scale
lambda = exp(log_lam)
range(lambda)
length(lambda)
```


```{r, results="hide"}
### LASSO algorithm - determine best function ###

# Specify initial B = 0 (due to first lambda = maximum lambda)
B.pkg = numeric(length = ncol(X.train))
# Initialize list that will store the B vectors for each lambda
results.pkg = list()
# For lambda max, B = 0
results.pkg[[1]] = B.pkg
# Initialize vector to store BIC values
bic.pkg = numeric(length(lambda))
bic.pkg[1] = 10^10 # Some arbitrarily large value

# Loop through all lambda values in Lasso function - see scratch work below for running only one iteration
time.start <- Sys.time()
for(l in 2:length(lambda)){
  # Calculate new B for specified lambda
  results.pkg[[l]] = staRz::LQA.lasso(X.train, Y=y.train, B.pkg, lambda[l], tol = 10^-4, max_it = 1000)
  # Update B with new value
  B.pkg = results.pkg[[l]]
  # Calculate EBIC - Extended BIC (Chen and Chen 2008)
  # Calculate linear predictor with updated B
  eta.pkg = X.train %*% B.pkg
  # EBIC = -2 log-lik + (num params) * log(num observations) + 2 * (num params) * constant * log(num total param - cols of X)
  bic.pkg[l] = -2 * sum( y.train * eta.pkg - log(1 + exp(eta.pkg)) ) +
    sum(B.pkg!=0) * log(nrow(X.train)) + 2 * sum(B.pkg!=0) * 0.5 * log(ncol(X.train))
  # Potential options for constant in last term: 0.25, 0.5, 1
  # According to Chen and Chen paper on extended BIC (EBIC)
  # 0.5 good option
  sprintf("l: %i", l)
}
time.stop <- Sys.time()
```

The `LQA.lasso` function was coded using the package `RcppArmadillo` to reduce computation time when identifiying the best model using the different values of $\lambda$. The function takes approximately 40 seconds to run.

```{r}
time.stop - time.start
```

From the output below, we see that the best model had $\lambda = 0.0044$ and resulted in five non-zero coefficients (excluding the intercept). Specifically, five of the six "human-made" variables had non-zero coefficients (all except for title word count), and the coefficients for all 25 of the "machine-made" LDA variables were zero.

```{r}
best.lambda <- lambda[ which.min(bic.pkg[-1]) ]
best.lambda
beta.coefs <- results.pkg[[ which.min(bic.pkg[-1]) ]]
row.names(beta.coefs) <- c("Intercept", colnames(X.train)[-1])
beta.coefs[beta.coefs != 0,]
```

We used these coefficients in a logistic regression model with the logit link to calculate the estimated probability that a review gave a 4- or 5-star rating. These probabilities were calculated for all observations in both the training and test sets. To determine the probability cutoff value when classifying reviews to a response of 1 or 0, we constructed confusion matrices for different cutoff values and calculated Cohen's kappa for each. We then optimized the cutoff by selecting the value that produced the highest kappa.

```{r, message=FALSE, warning=FALSE}
# Calculate probability of success for observations in test set
test.probs <- exp( X.test %*% beta.coefs ) / ( 1 + exp( X.test %*% beta.coefs ) )

# Function to create confustion matrix
conf.mat.fun <- function( y.vals, prob.vals, cutoff ){
  # Predict Y for given cutoff
  y.pred <- ifelse( prob.vals >= cutoff, 1, 0 )
  
  # Calculate number of true positives, true negatives, false positives, false negatives
  num.tp <- sum( ifelse( y.vals == 1 & y.pred == 1, 1, 0 ) )
  num.tn <- sum( ifelse( y.vals == 0 & y.pred == 0, 1, 0 ) )
  num.fp <- sum( ifelse( y.vals == 0 & y.pred == 1, 1, 0 ) )
  num.fn <- sum( ifelse( y.vals == 1 & y.pred == 0, 1, 0 ) )
  
  # Output confustion matrix (observed on rows, predicted on columns)
  #     1   0
  # 1   tp  fn
  # 0   fp  tn
  conf.mat <- matrix(0, nrow=2, ncol=2)
  conf.mat[1,1] <- num.tp
  conf.mat[1,2] <- num.fn
  conf.mat[2,1] <- num.fp
  conf.mat[2,2] <- num.tn
  
  return(conf.mat)
}

## Functions to find optimal cutoff for probabilities to predict class y = 1
## based on kappa (use training set)

# Function to get kappa score based on predicted probabilities and true classes
# with a specified cutoff value for predicting class y = 1
get.kappa <- function(x = 0.5, y, pred.prob) {
  library(psych)
  
  yhat <- ifelse(pred.prob > x, 1, 0)
  cm <- table("Predicted" = yhat, "Actual" = y)
  
  kappa <- cohen.kappa(cm)$kappa
  kappa
}

# Function to optimize the cutoff so that kappa is maximized
opt.cutoff <- function(y, pred.prob) {
  library(optimx)
  optimx(
    par = 0.5,                         ## initial value for cutoff
    fn = get.kappa,                    ## function to optimize
    method = "Nelder-Mead",            ## method for opt
    control = list(maximize = TRUE),
    y = y,                             ## plug in true classes
    pred.prob = pred.prob              ## plug in predicted probs
  )
}
# Training set probabilities
train.probs <- exp( X.train %*% beta.coefs ) / ( 1 + exp( X.train %*% beta.coefs ) )
best.cutoff <- opt.cutoff( y.train, train.probs )$p1
best.cutoff
```

The optimization function selected a cutoff of 0.842. Thus, reviews with a predicted probability greater than 0.842 were classified as $y=1$ (4 or 5 stars), and all other reviews were classified as $y=0$ (1, 2, or 3 stars).  Although this cutoff value was determined using the training set, we also use this classification rule with the predicted probabilities for the test set. The confusion matrix (reported below) resulted in $\kappa = 0.322$ and a misclassification rate of 0.090.

```{r}
# Cohen's kappa and misclassification error rate for test set using best cutoff value
test.conf.mat <- conf.mat.fun( y.test, test.probs, best.cutoff )
row.names(test.conf.mat) <- c("Observed: 1", "Observed: 0")
colnames(test.conf.mat) <- c("Predicted: 1", "Predicted: 0")
test.conf.mat
kappa.test <- cohen.kappa(test.conf.mat)$kappa
kappa.test
mcer.test <- (test.conf.mat[1,2] + test.conf.mat[2,1]) / length(y.test)
mcer.test
```

Next, we wanted to compare our LASSO function to the LASSO function provided by the package *glmnet*, which uses cross validation to select the optimal $\lambda$. Comparing 68 different values of $\lambda$, *glmnet* required approximately 7 seconds to run and selected $\lambda = 0.0013$. Applying a more lenient penalty with this lower $\lambda$ value, *glmnet* produced 18 non-zero coefficients, excluding the intercept (these coefficient values are presented later alongside the coefficients from the first model). However, all of the non-zero coefficients from *glmnet* that correpsond to zero coefficients from the *staRz* LASSO had a magnitude smaller than 0.065. To compare the prediction accuracy between the two LASSO functions, we used the same cutoff of 0.842 to classify the test set predictions using the best *glmnet* model, which resulted in $\kappa = 0.329$ and a misclassification rate of 0.093. Cohen's kappa was slightly higher for the *glmnet* model.

```{r, message=FALSE}
############################
### Comparison to glmnet ###
############################

# glmnet - find best model across different values of lambda
library(glmnet)
time.start <- Sys.time()
cv.glmmod <- cv.glmnet( X.train[,-1], y.train, alpha=1, family="binomial" )
best.lambda.glmnet <- cv.glmmod$lambda.min
glmmod <- glmnet( X.train[,-1], y.train, alpha=1, family="binomial" )
time.stop <- Sys.time()
time.stop - time.start

# glmnet results using best lambda = 0.0013
best.lambda.glmnet
glmnet.coefs <- coef(glmmod)[ , glmmod$lambda == best.lambda.glmnet ]

# glmnet predictions for test set using best lambda = 0.0013
glmnet.probs <- exp( X.test %*% glmnet.coefs ) / ( 1 + exp( X.test %*% glmnet.coefs ) )
glmnet.conf.mat <- conf.mat.fun( y.test, glmnet.probs, best.cutoff )
row.names(glmnet.conf.mat) <- c("Observed: 1", "Observed: 0")
colnames(glmnet.conf.mat) <- c("Predicted: 1", "Predicted: 0")
glmnet.conf.mat
kappa.glmnet <- cohen.kappa(glmnet.conf.mat)$kappa
kappa.glmnet
mcer.glmnet <- (glmnet.conf.mat[1,2] + glmnet.conf.mat[2,1]) / length(y.test)
mcer.glmnet
```

We also compared our LASSO function to the *glmnet* LASSO function using $\lambda = 0.0044$, the optimal $\lambda$ selected by our function ($\kappa = 0.328$, 0.087 misclassification rate). Consistent with the results from our first model, this *glmnet* model produced only five non-zero coefficients (excluding the intercept) that corresponded to the same "human-made" variables selected by the *staRz* model. The coefficients for both models had matching signs and similar magnitudes. The coefficients for all three LASSO models are listed below.

```{r}
########################################################
### Comparison to glmnet using staRz lambda = 0.0044 ###
########################################################

# glmnet results using staRz lambda = 0.0044
glmnet.coefs2 <- coef(glmmod)[ , glmmod$lambda == glmmod$lambda[30] ]

# glmnet predictions for test set using staRz lambda = 0.0044
glmnet.probs2 <- exp( X.test %*% glmnet.coefs2 ) / ( 1 + exp( X.test %*% glmnet.coefs2 ) )
glmnet.conf.mat2 <- conf.mat.fun( y.test, glmnet.probs2, best.cutoff )
kappa.glmnet2 <- cohen.kappa(glmnet.conf.mat2)$kappa
kappa.glmnet2
mcer.glmnet2 <- (glmnet.conf.mat2[1,2] + glmnet.conf.mat2[2,1]) / length(y.test)
mcer.glmnet2

# Comparison of coefficients for all three LASSO models
coefs.df <- data.frame( lasso = beta.coefs, glmnet1 = glmnet.coefs, glmnet2 = glmnet.coefs2 )
colnames(coefs.df) <- c("staRz (l=.0044)", "glmnet (l=.0013)", "glmnet (l=.0044)")
round(coefs.df, 3)
```

In addition to *glmnet*, we compare the predictive accuracy of our LASSO model to that of a stepwise regression model, a random forest using the entire training set, and a random forest using a subset of the training set in which the reviews with 4 or 5 stars were down-sampled to match the number of reviews with 1, 2, or 3 stars.



```{r, message=FALSE, results="hide", warning=FALSE}
###############################################################
### Run Stepwise regression and get predicted probabilities ###
###############################################################

# Re-format data
train2 <- as.data.frame(train[,-c(1,33)])
train2$y <- train[,33]
test2 <- as.data.frame(test[,-c(1,33)])
test2$y <- test[,33]

full.mod <- glm(
  y ~ .,
  data = train2,
  family = binomial
)

time.start <- Sys.time()
step.mod <- step(full.mod)
time.stop <- Sys.time()
time.step <- time.stop - time.start

# Get predicted probabilities and find optimal cutoff
step.pred.prob.train <- predict(step.mod, newdata = train2[,-32], type = "response")
step.pred.prob.test <- predict(step.mod, newdata = test2[,-32], type = "response")
step.cutoff <- opt.cutoff(y = train2$y, pred.prob = step.pred.prob.train)$p1

# Construct confusion matrix and calculate kappa and misclassification rate
step.conf.mat <- conf.mat.fun( test2$y, step.pred.prob.test, step.cutoff )
kappa.step <- cohen.kappa(step.conf.mat)$kappa
mcer.step <- (step.conf.mat[1,2] + step.conf.mat[2,1]) / length(y.test)


#########################################
### Random forest (full training set) ###
#########################################

# Random forest model
library(ranger)
time.start <- Sys.time()
rf.fit <- ranger(
  as.factor(y) ~ .,
  importance = "impurity",
  data = train2,
  num.trees = 2500
)
time.stop <- Sys.time()
time.rf1 <- time.stop - time.start

# Get predicted probabilities and find optimal cutoff
rf.train.pred <- predict(rf.fit, data = train2, predict.all = TRUE)
rf.test.pred <- predict(rf.fit, data = test2)$predictions
rf.train.pred.prob <- rowMeans(rf.train.pred$predictions - 1)
rf.cutoff <- opt.cutoff(y = train2$y, pred.prob = rf.train.pred.prob)$p1  ## gives 0.5

# Construct confusion matrix and calculate kappa and misclassification rate
rf.conf.mat1 <- conf.mat.fun( test2$y, as.numeric(as.character(rf.test.pred)), rf.cutoff )
kappa.rf1 <- cohen.kappa(rf.conf.mat1)$kappa
mcer.rf1 <- (rf.conf.mat1[1,2] + rf.conf.mat1[2,1]) / length(y.test)


######################################
### Random Forest w/ down sampling ###
######################################

# Down sample training set
y0.indx <- which(train2$y == 0)
y1.indx <- which(train2$y == 1)

set.seed(1341)
y1.select <- sample(y1.indx, size = length(y0.indx))
train0 <- train2[c(y0.indx, y1.select), ]

# Run random forest
start <- Sys.time()
rf0.fit <- ranger(
  as.factor(y) ~ .,
  importance = "impurity",
  data = train0,
  num.trees = 5000
)
end <- Sys.time()
time.rf2 <- end - start

# Get predicted probabilities
rf0.train.pred <- predict(rf0.fit, data = train2, predict.all = TRUE)
rf0.train.pred.prob <- rowMeans(rf0.train.pred$predictions - 1)

# Find optimal cutoff
rf0.cutoff <- opt.cutoff(y = train2$y, pred.prob = rf0.train.pred.prob)$p1

# Fit to test data
rf0.pred.prob <- rowMeans(predict(rf0.fit, data = test2, predict.all = TRUE)$predictions - 1)
rf0.pred <- ifelse(rf0.pred.prob > rf0.cutoff, 1, 0)

# Construct confusion matrix and calculate kappa and misclassification rate
rf.conf.mat2 <- conf.mat.fun( test2$y, as.numeric(as.character(rf0.pred.prob)), rf0.cutoff )
kappa.rf2 <- cohen.kappa(rf.conf.mat2)$kappa
mcer.rf2 <- (rf.conf.mat2[1,2] + rf.conf.mat2[2,1]) / length(y.test)
```

The stepwise model and the random forest using the full training set both required nearly 1.5 minutes to run. The random forest using the down-sampled training set ran in approximatley 11 seconds.

```{r}
# Computation time for stepwise model
time.step

# Computation time for random forest using full training set
time.rf1

# Computation time for random forest using down-sampled training set
time.rf2
```

The stepwise process selected 15 variables to include in the final model. Of these variables, 11 corresponded to non-zero coefficients in the first *glmnet* LASSO model which selected $\lambda$ via cross validation.

In the classification process for the stepwise model and both random forests, the optimal cutoff values was found for each model type. These cutoffs for these three models are listed below:

```{r}
rbind(step.cutoff, rf.cutoff, rf0.cutoff)
```

Finally, we compare the predictive accuracy of all models using Cohen's kappa. From the table below, we see that the model with the highest kappa is the stepwise model, although the *staRz* LASSO and both *glmnet* LASSO models had kappa values only slightly lower. Both of the random forest models had the lowest kappa values, with the random forest using the full training set having the worst predictive accuracy according to the highest-kappa criterion.

```{r}
kappa.vals <- cbind(kappa.test, kappa.glmnet, kappa.glmnet2, kappa.step, kappa.rf1, kappa.rf2)
mcr.vals <-  cbind(mcer.test, mcer.glmnet, mcer.glmnet2, mcer.step, mcer.rf1, mcer.rf2)
comp.vals <- rbind(kappa.vals, mcr.vals)
colnames(comp.vals) <- c("staRz (l=.0044)", "glmnet (l=.0013)", "glmnet (l=.0044)", "stepwise", "rf (full)", "rf (down)")
row.names(comp.vals) <- c("Cohen's kappa", "misclassification rate")
round(comp.vals, 4)


## Create kappa plot
kappa.vals2 <- rbind(kappa.test, kappa.glmnet, kappa.glmnet2, kappa.step, kappa.rf1, kappa.rf2)
rownames(kappa.vals2) <- c("staRz", "glmnet (l=.0013)", "glmnet (l=.0044)", "Stepwise", "RF (full)", "RF (down)")
colnames(kappa.vals2) <- "Kappa"
kappa.vals2 <- as.data.frame(kappa.vals2)
kappa.vals2$Method <- rownames(kappa.vals2)

library(ggplot2)
p1 <- ggplot(kappa.vals2, aes(
  x = reorder(Method, -Kappa), 
  y = Kappa, 
  fill = as.factor(Method))
) +
  geom_bar(stat = "identity") + 
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  ) + 
  ggtitle("Comparison of Kappa Across Methods")
p1
```


## Discussion

The major difference between the LASSO function in our *staRz* package and the *glmnet* LASSO is the method in which the optimal value of $\lambda$ is selected. Our function uses EBIC which penalizes higher dimensionality, resulting in a smaller optimal $\lambda$ and fewer non-zero coefficients. Although our algorithm requires more computation time, the coefficient estimates are similar to *glmnet* for a fixed $\lambda$ value, and the prediction accuracy for our model was only slightly lower than *glmnet* models in the examples tested.

Although the stepwise model produced the highest value of Cohen's kappa, it has a higher computation time than any of the LASSO models and has more variables in the final model than our *staRz* LASSO model. If a simple model is desired, then we recommend the *staRz* LASSO model which does not require any "machine-made" covariates obtained using LDA.

These models may not be generalizable to reviews for other products, however, it is worth noting that only the *staRz* model can be used for prediction and classification of additional reviews that were not included in the training and test sets. All other models rely on the LDA variables to some extent, and any additional reviews---whether or not they are for the same products as the reviews in our training/test sets---do not have LDA variables. In order to use these additional reviews, LDA would have to be applied using the entire training and test sets together along with the new reviews, and the models would have to be refit. Since the *staRz* LASSO excludes the LDA variables in the final model, only the "human-made" variables would have to be calculated for each new review.



### Limitations

There were several limitations to our project. Most came down to our created variables (machine- and human-made) and the amount of time we had.

#### LDA

LDA assumes number of topics is fixed and known in advance. We may have chosen the wrong number of topics for reviews.text or reviews.title. For simplicity of this project, we decided to look at 20 topics for reviews.text and 5 topics for reviews.title, but it probably would have been good for us to look at a variety of choices.

Another issue is that we used a unigram model in which only one word can go in a topic. This does not account for reviews such as “this product is not good” or “I don’t hate this product”, in which "not good" should be deemed negative and "don't hate" should be deemed positive. If we had more time, we could have tried a bi-gram, or even an N-gram model but this increases computational complexity. Another option would be to use HMM-LDA.

#### Human-Made Variables

For creating the positive and negative variables, one issue is that sentiment does not account for sentence context. For example, in the sentence "the miniature zen garden helped me to slow down and relax”, "slow" should be considered a positive word based on the product, but it is considered a negative word by our function. One way we could fix this is by defining the vector of positive and negative words based on the product itself.

Another issue in creating the reviews.title and reviews.text related variables is that they do not
not account for spelling or typing errors. For examples, in the sentece “simply does everything I need. Thank youAnd silk works wonders”, "youAnd" would be considered one word instead of two.

The human-made review variables are inherently related to the LDA topics, we just don't know how they are related. This could have been why our LASSO model got rid of all of the LDA topics. If we only included the LDA topics as covariates, more of them may have been included in the final model.

Lastly, our data may not be the most representative as a large majority of our observations (90+%) have a dichotomized rating of 1. This is likely why we had so many observations that did not have any negative words in the reviews.title or reviews.text. If we had more time, we would look for other products with a lower average rating to analyze (ie 3.5, 2.7), or products that had a better balance of ratings.

Alternatively, we could have dichotomized ratings differetly. We see below that over 68% of the observations have a rating of 5. It might have been better if we dichotomized the ratings so that ratings of 5 get a value of 1 and all others get a value of 0. If we had done this, our data would have been more balanced and our results may have been better.

```{r}
round(table(clean_reviews$reviews.rating)/denom,3)
```



